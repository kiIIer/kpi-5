# Лабораторна робота №4

## Тема

Моделювання функції двох змінних з двома входами і одним виходом на основі нейронних мереж

## Мета

Дослідити структуру та принцип роботи нейронної мережі. За допомогою нейронної мережі змоделювати функцію двох змінних

## Виконання

Код можна переглянути тут: [[net-compare]](net-compare)

Я використав tensorflow для створення та навчання моделей, обрав функцію двох змінних z = x^2 + y, та знегерував за допомогою неї датасет. Далі створив моделі як вказано в завданні та створив графіки для їх порівняння та вивід у консоль їх результатів

### Результати

![Рисунок 1 - Точність моделей протягом епох](stats.png)

Статистика після навчання:

Training Feedforward Model A (10 Neurons)...
Model: Feedforward Model A (10 Neurons)

- Final Training Loss: 8.1465
- Final Validation Loss: 7.8444

Training Feedforward Model B (20 Neurons)...
Model: Feedforward Model B (20 Neurons)

- Final Training Loss: 3.8610
- Final Validation Loss: 3.8992

Training Cascade Model A (20 Neurons)...
Model: Cascade Model A (20 Neurons)

- Final Training Loss: 4.1171
- Final Validation Loss: 4.1457

Training Cascade Model B (2x10 Neurons)...
Model: Cascade Model B (2x10 Neurons)

- Final Training Loss: 1.5211
- Final Validation Loss: 1.4862

Training Elman Model A (15 Neurons)...
Model: Elman Model A (15 Neurons)

- Final Training Loss: 4.5959
- Final Validation Loss: 4.4918

Training Elman Model B (3x5 Neurons)...
Model: Elman Model B (3x5 Neurons)

- Final Training Loss: 1.2494
- Final Validation Loss: 1.2702

### Порівняння

На основі наведених статистичних даних можна зробити такі висновки про кожну з моделей:

1. **Feedforward Model A (10 Neurons)**
   - Кінцева помилка на тренувальному наборі: 5.8748
   - Кінцева помилка на валідаційному наборі: 6.7596
   - Ця модель має просту структуру із замалою кількістю нейронів у єдиному прихованому шарі, що може бути не достатньо для складніших задач.

2. **Feedforward Model B (20 Neurons)**
   - Кінцева помилка на тренувальному наборі: 5.2039
   - Кінцева помилка на валідаційному наборі: 6.1999
   - Збільшення кількості нейронів у прихованому шарі покращило результати порівняно з попередньою моделлю, свідчачи про те, що додаткові нейрони допомагають у навчанні.

3. **Cascade Model A (20 Neurons)**
   - Кінцева помилка на тренувальному наборі: 4.2417
   - Кінцева помилка на валідаційному наборі: 4.9140
   - Модель з каскадним зворотнім розповсюдженням показує кращі результати, що може свідчити про ефективність цієї архітектури для даної задачі.

4. **Cascade Model B (2x10 Neurons)**
   - Кінцева помилка на тренувальному наборі: 1.5563
   - Кінцева помилка на валідаційному наборі: 1.8389
   - Розбиття 20 нейронів на два приховані шари значно покращило результати, можливо завдяки більш глибокому представленню даних.

5. **Elman Model A (15 Neurons)**
   - Кінцева помилка на тренувальному наборі: 6.4774
   - Кінцева помилка на валідаційному наборі: 7.5862
   - Хоча Елмановська модель використовує рекурентні нейрони, результати не є найкращими для цієї конкретної задачі.

6. **Elman Model B (3x5 Neurons)**
   - Кінцева помилка на тренувальному наборі: 3.2937
   - Кінцева помилка на валідаційному наборі: 3.9300
   - Розподіл нейронів по трьох шарах в рекурентній мережі показав кращі результати, ніж одношарова конфігурація, але все ще залишається позаду каскадної моделі з двома шарами.

#### Порівняльний Аналіз

- Моделі з каскадним зворотнім розповсюдженням (Cascade Model B) п

оказали найкращі результати, що свідчить про високу ефективність глибоких мереж для цієї задачі.

- Збільшення кількості нейронів у моделях feedforward (Feedforward Model B) також призвело до покращення результатів, але не настільки виразно, як у каскадних моделей.
- Елмановські моделі (Elman Model B), хоч і мають рекурентну структуру, не показали найкращих результатів для цієї задачі. Однак, вони можуть бути більш ефективними в задачах, де важливий часовий аспект даних.

## Висновок

На цій лабораторній роботі я попрацював з трьому різними типами мереж з трохи різними налаштуваннями та знаходив найліпру для завдання апроксимації функції функції двох змінних.
